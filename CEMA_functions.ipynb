{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c5be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytrinamic\n",
    "from pytrinamic.connections import ConnectionManager\n",
    "from pytrinamic.modules import TMCM6110\n",
    "import time\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import trange\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55558cc3",
   "metadata": {},
   "source": [
    "# Motor Control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XYZ():\n",
    "    def __init__(self):\n",
    "        self.connectionManager = ConnectionManager()\n",
    "        self.interface = self.connectionManager.connect()\n",
    "\n",
    "        # Create an instance of the TMCM_6110 class\n",
    "        self.module = TMCM6110(self.interface)\n",
    "\n",
    "\n",
    "        self.motor_0 =  self.module.motors[0]\n",
    "        self.motor_1 =  self.module.motors[1]\n",
    "        self.motor_2 =  self.module.motors[2]\n",
    "        print(\"Preparing parameters\")\n",
    "        \n",
    "    def XYZ_setup(self,max_current=500,standby_current=200,boost_current=0,velocity=1500,acceleration=1000,position=0):\n",
    "        motors_list = []\n",
    "        for i in [0,1,2]:\n",
    "            motor_name = f\"motor_{i}\"\n",
    "            motor = getattr(self, motor_name)\n",
    "            # Now you can use 'motor' as a reference to self.motor_0, self.motor_1, etc.\n",
    "            motor.drive_settings.max_current=max_current\n",
    "            motor.drive_settings.standby_current=standby_current\n",
    "            motor.drive_settings.boost_current=boost_current\n",
    "            motor.drive_settings.microstep_resolution = motor.ENUM.microstep_resolution_256_microsteps\n",
    "            motor.max_acceleration=acceleration\n",
    "            motor.max_velocity=velocity\n",
    "#             motor.actual_position=position\n",
    "            print(motor)\n",
    "            motors_list.append(motor)\n",
    "        return motors_list[2],motors_list[1],motors_list[0],self.interface\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9c4efd",
   "metadata": {},
   "source": [
    "# Capturing functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d31bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_location(scope, pt_exp, exp_name, x, y, num=1000):\n",
    "    \"\"\"\n",
    "    Captures and stores traces for a given (x, y) location.\n",
    "\n",
    "    Parameters:\n",
    "    scope: Object responsible for capturing traces.\n",
    "    pt_exp: Experiment dataset handler for plaintexts and keys.\n",
    "    exp_name: Dataset handler where captured traces will be stored.\n",
    "    x, y: Coordinates representing the capture location.\n",
    "    num (int, optional): Number of traces to capture (default is 1000).\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve key, random plaintext, and fixed plaintext datasets\n",
    "    keys_pt = pt_exp.get_dataset(\"keys\").read_data(0, num)\n",
    "    random_pt = pt_exp.get_dataset(\"plaintexts\").read_data(0, num)\n",
    "    fixed_pt = pt_exp.get_dataset(\"fixed_pt\").read_data(0, num)\n",
    "\n",
    "    # Capture traces using test vector leakage assessment (TVLA) method\n",
    "    f, r = scope.capture_traces_tvla(num, keys_pt, fixed_pt, keys_pt, random_pt)\n",
    "\n",
    "    # Store captured traces for the given location\n",
    "    print(\"Storing for location: \" + str(x) + \"_\" + str(y))\n",
    "    exp_name.add_dataset(\"fixed_\" + str(x) + \"_\" + str(y), f, datatype=\"float32\")\n",
    "    exp_name.add_dataset(\"random_\" + str(x) + \"_\" + str(y), r, datatype=\"float32\")\n",
    "\n",
    "    print(\"Traces stored\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def Grid_Tracing_scapegoat(X_range, Y_range, X_number_of_step, Y_number_of_step, X, Y, Z, interface, scope, pt_exp, exp_store, number_of_traces):\n",
    "    \"\"\"\n",
    "    Performs grid-based scanning and captures traces at each step.\n",
    "\n",
    "    Parameters:\n",
    "    X_range, Y_range: Step sizes for movement in X and Y directions.\n",
    "    X_number_of_step, Y_number_of_step: Number of steps to take in X and Y directions.\n",
    "    X, Y, Z: Actuators controlling movement along respective axes.\n",
    "    interface: Communication interface for device control.\n",
    "    scope: Object responsible for capturing traces.\n",
    "    pt_exp: Experiment dataset handler for plaintexts and keys.\n",
    "    exp_store: Experiment object to store captured traces.\n",
    "    number_of_traces: Number of traces to capture at each grid point.\n",
    "    \"\"\"\n",
    "\n",
    "    cordinate_traces = {}  # Dictionary to store traces at different coordinates\n",
    "    X_moment = 0\n",
    "    Y_moment = 0\n",
    "\n",
    "    # Capture initial location traces\n",
    "    capture_location(scope, pt_exp, exp_store, X_moment, Y_moment, number_of_traces)\n",
    "\n",
    "    # Store initial positions\n",
    "    X_start_position = X.get_actual_position()\n",
    "    Y_start_position = Y.get_actual_position()\n",
    "    print(f\"Starting Position - ({X_start_position}, {Y_start_position})\")\n",
    "\n",
    "    # Perform scanning along the Y-axis\n",
    "    while Y_moment <= Y_number_of_step:\n",
    "        X_initial_position = X.get_actual_position()\n",
    "        Y_initial_position = Y.get_actual_position()\n",
    "\n",
    "        # Move in the positive X direction\n",
    "        for _ in range(X_number_of_step):\n",
    "            X.move_by(X_range)\n",
    "            print(f'Moving X to {X_initial_position + X_range}')\n",
    "            while X.get_actual_position() != X_initial_position + X_range:\n",
    "                time.sleep(0.1)  # Wait until movement is complete\n",
    "            X_moment += 1\n",
    "            capture_location(scope, pt_exp, exp_store, X_moment, Y_moment, number_of_traces)\n",
    "            X_initial_position = X.get_actual_position()\n",
    "\n",
    "        if Y_moment == Y_number_of_step:\n",
    "            break  # Stop if the last Y step is reached\n",
    "\n",
    "        # Move in the positive Y direction\n",
    "        Y.move_by(Y_range)\n",
    "        Y_moment += 1\n",
    "        print(f'Moving Y to {Y_initial_position + Y_range}')\n",
    "        while Y.get_actual_position() != Y_initial_position + Y_range:\n",
    "            time.sleep(0.1)  # Wait until movement is complete\n",
    "        capture_location(scope, pt_exp, exp_store, X_moment, Y_moment, number_of_traces)\n",
    "        Y_initial_position = Y.get_actual_position()\n",
    "\n",
    "        # Move in the negative X direction\n",
    "        for _ in range(X_number_of_step):\n",
    "            X.move_by(-X_range)\n",
    "            print(f'Moving X to {X_initial_position - X_range}')\n",
    "            while X.get_actual_position() != X_initial_position - X_range:\n",
    "                time.sleep(0.1)  # Wait until movement is complete\n",
    "            X_moment -= 1\n",
    "            capture_location(scope, pt_exp, exp_store, X_moment, Y_moment, number_of_traces)\n",
    "            X_initial_position = X.get_actual_position()\n",
    "\n",
    "        if Y_moment == Y_number_of_step:\n",
    "            break  # Stop if the last Y step is reached\n",
    "\n",
    "        # Move in the positive Y direction again\n",
    "        Y.move_by(Y_range)\n",
    "        Y_moment += 1\n",
    "        print(f'Moving Y to {Y_initial_position + Y_range}')\n",
    "        while Y.get_actual_position() != Y_initial_position + Y_range:\n",
    "            time.sleep(0.1)  # Wait until movement is complete\n",
    "        capture_location(scope, pt_exp, exp_store, X_moment, Y_moment, number_of_traces)\n",
    "        Y_initial_position = Y.get_actual_position()\n",
    "\n",
    "    # Return to the starting position\n",
    "    X.move_to(X_start_position)\n",
    "    while X.get_actual_position() != X_start_position:\n",
    "        time.sleep(0.1)  # Wait until movement is complete\n",
    "\n",
    "    Y.move_to(Y_start_position)\n",
    "    while Y.get_actual_position() != Y_start_position:\n",
    "        time.sleep(0.1)  # Wait until movement is complete\n",
    "\n",
    "    print(f\"Final Position - ({X.get_actual_position()}, {Y.get_actual_position()})\")\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ede17",
   "metadata": {},
   "source": [
    "# Metrics (one-click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81669bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_CEMA_heatmap(test, pt_exp, num, target_byte=0, grid_size=5):\n",
    "    \"\"\"\n",
    "    Compute and visualize Correlation Electromagnetic Analysis (CEMA) results as a heatmap.\n",
    "\n",
    "    Parameters:\n",
    "    - test: An object that provides access to trace datasets.\n",
    "    - pt_exp: Experiment dataset handler for keys and plaintexts.\n",
    "    - num: Number of traces to process.\n",
    "    - target_byte: Byte index for correlation analysis (default is 0).\n",
    "    - grid_size: The size of the grid (default is 5x5).\n",
    "    \n",
    "    Returns:\n",
    "    - CEMA_guesses_rotated: Rotated array of best key guesses from CEMA.\n",
    "    - CEMA_values_rotated: Rotated array of maximum correlation values from CEMA.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve keys and plaintext datasets\n",
    "    keys = pt_exp.get_dataset(\"keys\").read_data(0, num)\n",
    "    plaintexts = pt_exp.get_dataset(\"plaintexts\").read_data(0, num)\n",
    "\n",
    "    # Initialize arrays to store CEMA results\n",
    "    CEMA_values = np.zeros((grid_size, grid_size))  # Stores maximum correlation values\n",
    "    CEMA_guesses = np.zeros((grid_size, grid_size))  # Stores corresponding key guesses\n",
    "\n",
    "    # Perform CEMA analysis across the grid\n",
    "    for i in range(grid_size):\n",
    "        for j in trange(grid_size):\n",
    "            print(f\"Processing grid position ({i}, {j})\")\n",
    "\n",
    "            # Retrieve traces for the current grid position\n",
    "            traces = test.get_dataset(f\"random_{i}_{j}\").read_data(0, num)\n",
    "\n",
    "            # Perform CEMA to obtain correlation values and best key guess\n",
    "            best_guess, max_correlation = scapegoat_cpa_byte(traces, keys, plaintexts, target_byte)\n",
    "\n",
    "            # Store results\n",
    "            CEMA_values[i, j] = max_correlation\n",
    "            CEMA_guesses[i, j] = best_guess\n",
    "\n",
    "    # Rotate the heatmap for correct visualization\n",
    "    CEMA_values_rotated = np.rot90(CEMA_values, k=3)  # Rotate by 90 degrees clockwise\n",
    "    CEMA_guesses_rotated = np.rot90(CEMA_guesses, k=3)\n",
    "\n",
    "    # Create a heatmap visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(CEMA_values_rotated, annot=True, cbar=True, square=True)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title(\"CEMA Heatmap\")\n",
    "    plt.xlabel(\"Grid Column (j)\")\n",
    "    plt.ylabel(\"Grid Row (i)\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    return CEMA_guesses_rotated, CEMA_values_rotated\n",
    "\n",
    "\n",
    "\n",
    "def plot_SNR_heatmap(test, pt_exp, num, target_byte=0, grid_size=5, SNR_type=\"BYTE\"):\n",
    "    \"\"\"\n",
    "    Compute and visualize Signal-to-Noise Ratio (SNR) results as a heatmap.\n",
    "\n",
    "    SNR Types:\n",
    "    - \"BYTE\": Uses all possible byte combinations.\n",
    "    - \"FULL\": Uses all possible 16-byte key combinations.\n",
    "    - \"HW\": Uses the Hamming weight of the byte.\n",
    "\n",
    "    Parameters:\n",
    "    - test: An object that provides access to trace datasets.\n",
    "    - pt_exp: Experiment dataset handler for keys and plaintexts.\n",
    "    - num: Number of traces to process.\n",
    "    - target_byte: Byte index for SNR analysis (default is 0).\n",
    "    - grid_size: The size of the grid (default is 5x5).\n",
    "    - SNR_type: Type of SNR analysis (\"BYTE\", \"FULL\", or \"HW\").\n",
    "\n",
    "    Returns:\n",
    "    - SNR_values_rotated: Rotated array of maximum SNR values.\n",
    "    - SNR_dB: SNR values in decibels (10 * log10 of SNR_values_rotated).\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve keys and plaintext datasets\n",
    "    keys = pt_exp.get_dataset(\"keys\").read_data(0, num)\n",
    "    plaintexts = pt_exp.get_dataset(\"plaintexts\").read_data(0, num)\n",
    "\n",
    "    # Select the labeling method based on SNR type\n",
    "    if SNR_type == \"BYTE\":\n",
    "        labels = sbox_snr(num_traces=num, plaintexts=plaintexts, subkey_guess=keys, target_byte=target_byte)\n",
    "    elif SNR_type == \"HW\":\n",
    "        labels = leakage_model_hamming_weight_snr(num_traces=num, plaintexts=plaintexts, subkey_guess=keys, target_byte=target_byte)\n",
    "    elif SNR_type == \"FULL\":\n",
    "        labels = Sbox[plaintexts ^ keys]\n",
    "        labels = labels[:, 1]  # Extract the relevant labels for SNR computation\n",
    "    else:\n",
    "        print(\"Incorrect SNR type specified.\")\n",
    "        return -1\n",
    "\n",
    "    # Initialize array to store SNR values\n",
    "    SNR_values = np.zeros((grid_size, grid_size))\n",
    "\n",
    "    # Get unique label values\n",
    "    labels_unique = np.unique(labels)\n",
    "\n",
    "    # Perform SNR analysis across the grid\n",
    "    for i in range(grid_size):\n",
    "        for j in trange(grid_size):\n",
    "            sorted_labels = {k: [] for k in labels_unique}  # Dictionary to store traces per label\n",
    "            print(f\"Processing grid position ({i}, {j})\")\n",
    "\n",
    "            # Retrieve traces for the current grid position\n",
    "            traces = test.get_dataset(f\"random_{i}_{j}\").read_data(0, num)\n",
    "\n",
    "            # Organize traces according to their label\n",
    "            for index, label in enumerate(labels):\n",
    "                sorted_labels[label].append(np.array(traces[index]))\n",
    "\n",
    "            # Compute SNR for the given labels and traces\n",
    "            snr_result = signal_to_noise_ratio(sorted_labels)\n",
    "\n",
    "            # Store the maximum absolute SNR value\n",
    "            SNR_values[i, j] = np.nanmax(np.abs(snr_result))\n",
    "\n",
    "    # Rotate the heatmap for correct visualization\n",
    "    SNR_values_rotated = np.rot90(SNR_values, k=3)  # Rotate by 90 degrees clockwise\n",
    "\n",
    "    # Create a heatmap visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(SNR_values_rotated, annot=True, cbar=True, square=True)\n",
    "\n",
    "    # Add labels and title based on SNR type\n",
    "    plt.title(f\"SNR Heatmap ({SNR_type} mode)\")\n",
    "    plt.xlabel(\"Grid Column (j)\")\n",
    "    plt.ylabel(\"Grid Row (i)\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Compute SNR in decibels\n",
    "    SNR_dB = 10 * np.log10(SNR_values_rotated)\n",
    "\n",
    "    return SNR_values_rotated, SNR_dB\n",
    "\n",
    "\n",
    "def plot_t_statistic_heatmap(test, grid_size=5):\n",
    "    \"\"\"\n",
    "    Compute and visualize t-statistics as a heatmap.\n",
    "\n",
    "    This function calculates t-statistics for each position in a grid \n",
    "    and generates a heatmap representation.\n",
    "\n",
    "    Parameters:\n",
    "    - test: An object that has a `calculate_t_test` method to compute the t-statistics.\n",
    "    - grid_size: The size of the grid (default is 5x5).\n",
    "\n",
    "    Returns:\n",
    "    - t_values_rotated: Rotated array of maximum absolute t-statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the t-statistics array\n",
    "    t_values = np.zeros((grid_size, grid_size))\n",
    "\n",
    "    # Compute t-statistics for each grid position\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            t_stat, t_max = test.calculate_t_test(f\"fixed_{i}_{j}\", f\"random_{i}_{j}\")\n",
    "            t_values[i, j] = np.nanmax(np.abs(t_stat))  # Store the maximum absolute t-statistic\n",
    "\n",
    "    # Rotate the heatmap for correct visualization\n",
    "    t_values_rotated = np.rot90(t_values, k=3)  # Rotate by 90 degrees clockwise\n",
    "\n",
    "    # Create a heatmap using seaborn\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(t_values_rotated, annot=True, cbar=True, square=True)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title(\"Heatmap of t-statistics\")\n",
    "    plt.xlabel(\"Grid Column (j)\")\n",
    "    plt.ylabel(\"Grid Row (i)\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    return t_values_rotated\n",
    "\n",
    "\n",
    "def generate_box_plots(test, pt_exp, num_list, target_byte=0, grid_size=5):\n",
    "    \"\"\"\n",
    "    Generate box plots for SNR values at different numbers of traces.\n",
    "\n",
    "    This function computes SNR values for various trace counts and visualizes them\n",
    "    using box plots to analyze variations in SNR across different datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - test: An object that provides the `plot_SNR_heatmap_byte_bp` function.\n",
    "    - pt_exp: Experiment data handler.\n",
    "    - num_list: A list of trace counts to evaluate.\n",
    "    - target_byte: The target byte used for SNR calculation.\n",
    "    - grid_size: The grid size for SNR calculations (default is 5x5).\n",
    "\n",
    "    Returns:\n",
    "    - all_values: A list containing SNR value lists for each trace count.\n",
    "    \"\"\"\n",
    "\n",
    "    all_values = []  # Stores separate lists of SNR values for each trace count\n",
    "    labels = []  # Labels corresponding to each dataset\n",
    "\n",
    "    for num in num_list:\n",
    "        print(f\"Processing num_traces = {num}...\")\n",
    "\n",
    "        # Compute the SNR values for the given number of traces\n",
    "        SNR_values_rotated = plot_SNR_heatmap_byte_bp(test, pt_exp, num, target_byte, grid_size)\n",
    "\n",
    "        # Validate and flatten SNR values\n",
    "        if isinstance(SNR_values_rotated, list):\n",
    "            all_values.append(list(chain.from_iterable(SNR_values_rotated)))  # Flatten and store values\n",
    "        else:\n",
    "            print(f\"Invalid data format for num_traces={num}: {SNR_values_rotated}\")\n",
    "            continue\n",
    "\n",
    "        labels.append(f\"{num} traces\")  # Create labels for the box plot\n",
    "\n",
    "    # Generate box plot if valid data is available\n",
    "    if all_values:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(data=all_values)\n",
    "\n",
    "        # Configure plot labels and title\n",
    "        plt.xticks(ticks=range(len(num_list)), labels=labels)\n",
    "        plt.xlabel(\"Number of Traces\")\n",
    "        plt.ylabel(\"SNR Values\")\n",
    "        plt.title(\"Box Plot of SNR Values for Different Trace Counts\")\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No valid data to plot.\")\n",
    "\n",
    "    return all_values\n",
    "\n",
    "    \n",
    "\n",
    "def plot_CEMA_wr(test, pt_exp, num, target_byte=0, x=0, y=0, div=10):\n",
    "    \"\"\"\n",
    "    Generate a CPA correlation plot comparing the correct key vs. wrong keys over increasing trace counts.\n",
    "\n",
    "    This function performs Correlation Power Analysis (CPA) across different numbers of traces, \n",
    "    visualizing how the correct key and wrong keys' correlation evolve.\n",
    "\n",
    "    Parameters:\n",
    "    - test: An object that provides trace datasets.\n",
    "    - pt_exp: Experiment data handler providing plaintext and key datasets.\n",
    "    - num: Total number of traces to analyze.\n",
    "    - target_byte: The target byte index in the key (default is 0).\n",
    "    - x, y: Grid position for selecting the dataset.\n",
    "    - div: Step size for processing traces in intervals.\n",
    "\n",
    "    Returns:\n",
    "    - maxcpa_matrix: A matrix storing the maximum CPA correlation values for all 256 key guesses.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load key and plaintext data\n",
    "    keys = pt_exp.get_dataset(\"keys\").read_data(0, num)\n",
    "    plaintexts = pt_exp.get_dataset(\"plaintexts\").read_data(0, num)\n",
    "\n",
    "    # Load power traces for the selected grid position\n",
    "    traces = test.get_dataset(f\"random_{x}_{y}\").read_data(0, num)\n",
    "\n",
    "    # Initialize a matrix to store max CPA values across different key guesses\n",
    "    maxcpa_matrix = np.zeros((int(num / div), 256))\n",
    "\n",
    "    iterations = 1\n",
    "    for i in trange(1, num):\n",
    "        if i % div == 0:\n",
    "            for k in range(256):\n",
    "                # Compute leakage model using Hamming weight\n",
    "                leakage = leakage_model_hamming_weight(num_traces=i, plaintexts=plaintexts, subkey_guess=k, target_byte=target_byte)\n",
    "                \n",
    "                # Compute Pearson correlation\n",
    "                correlation = pearson_correlation(leakage, traces[:i])\n",
    "\n",
    "                # Store max correlation value for this key guess\n",
    "                maxcpa_matrix[int(i / div), k] = np.nanmax(np.abs(correlation))\n",
    "            \n",
    "            iterations += 1\n",
    "\n",
    "    # Debugging: Check matrix shape\n",
    "    print(\"Shape of maxcpa_matrix:\", maxcpa_matrix.shape)\n",
    "\n",
    "    # Prepare x-axis values (trace count steps)\n",
    "    xp = np.arange(2, min(iterations + 2, maxcpa_matrix.shape[0]))\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    if maxcpa_matrix.shape[0] > 2:\n",
    "        # Plot the statistical threshold\n",
    "        plt.plot(xp, (abs(4) / np.sqrt(xp * div)) * np.ones_like(xp), \n",
    "                 color=\"black\", linestyle='dotted', linewidth=1.5, label=\"Threshold\")\n",
    "\n",
    "        # Plot CPA correlations for all 256 key hypotheses\n",
    "        for i in range(256):\n",
    "            if i == 43:  # Assuming 43 is the correct key\n",
    "                plt.plot(xp, maxcpa_matrix[2:len(xp) + 2, i], color=\"red\",\n",
    "                         alpha=0.9, linewidth=1.5, label=\"Correct key\")\n",
    "            else:\n",
    "                plt.plot(xp, maxcpa_matrix[2:len(xp) + 2, i], color=\"grey\",\n",
    "                         alpha=0.1, linewidth=0.5, label=\"Wrong keys\" if i == 0 else \"\")\n",
    "\n",
    "    # Configure plot labels and title\n",
    "    plt.xlabel(f\"No. of traces Ã— {div}\", fontsize=14)\n",
    "    plt.ylabel(\"Max CPA Value\", fontsize=14)\n",
    "    plt.title(f\"Correlation Power Analysis: ({x}, {y})\", fontsize=18)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    return maxcpa_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd1b72",
   "metadata": {},
   "source": [
    "## helper functions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba461fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scapegoat_cpa(experiment):\n",
    "    \"\"\"\n",
    "    Perform Correlation Power Analysis (CPA) on an experiment dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - experiment: An object containing datasets for traces, keys, and plaintexts.\n",
    "\n",
    "    Returns:\n",
    "    - best_guess: List of best key byte guesses (one per byte position).\n",
    "    - cpa_refs: List of highest correlation values for each key byte position.\n",
    "    \"\"\"\n",
    "    num_bytes = 16  # AES has 16 key bytes\n",
    "    max_cpa = np.zeros(256)  # Store CPA values for each subkey guess\n",
    "    cpa_refs = np.zeros(num_bytes)  # Highest CPA values per key byte\n",
    "    best_guess = np.zeros(num_bytes, dtype=int)  # Best key guesses\n",
    "\n",
    "    # Load datasets\n",
    "    traces = experiment.get_dataset(\"CW_Capture_Traces\").read_all()\n",
    "    keys = experiment.get_dataset(\"CW_Capture_Keys\").read_all()\n",
    "    plaintexts = experiment.get_dataset(\"CW_Capture_Plaintexts\").read_all()\n",
    "\n",
    "    # Perform CPA attack for each byte in the key\n",
    "    for byte_idx in trange(num_bytes, desc=\"CPA on key bytes\"):\n",
    "        for k in range(256):\n",
    "            # Compute leakage model\n",
    "            leakage = leakage_model_hamming_weight(\n",
    "                num_traces=len(plaintexts),\n",
    "                plaintexts=plaintexts,\n",
    "                subkey_guess=k,\n",
    "                target_byte=byte_idx\n",
    "            )\n",
    "            # Compute Pearson correlation\n",
    "            correlation = pearson_correlation(leakage, traces)\n",
    "            max_cpa[k] = np.nanmax(np.abs(correlation))\n",
    "\n",
    "        # Store best guess and highest CPA value for this byte position\n",
    "        best_guess[byte_idx] = np.argmax(max_cpa)\n",
    "        cpa_refs[byte_idx] = np.nanmax(max_cpa)\n",
    "\n",
    "    return best_guess, cpa_refs\n",
    "\n",
    "def scapegoat_cpa_byte(traces, keys, plaintexts, target_byte):\n",
    "    \"\"\"\n",
    "    Perform Correlation Power Analysis (CPA) on a specific key byte to guess the subkey.\n",
    "\n",
    "    Parameters:\n",
    "    - traces: The captured power traces.\n",
    "    - keys: The actual secret keys corresponding to the traces.\n",
    "    - plaintexts: The plaintexts used for the power analysis.\n",
    "    - target_byte: The index of the target byte in the key to analyze.\n",
    "\n",
    "    Returns:\n",
    "    - best_guess: The best guess for the target key byte.\n",
    "    - cpa_ref: The highest correlation value obtained for the target byte.\n",
    "    \"\"\"\n",
    "    max_cpa = np.zeros(256)  # Store maximum CPA values for each possible subkey guess\n",
    "    cpa_ref = 0  # Store highest correlation value for the target byte\n",
    "    best_guess = 0  # Store best subkey guess for the target byte\n",
    "\n",
    "    # Perform CPA attack for each possible subkey guess (0-255)\n",
    "    for k in range(256):\n",
    "        # Compute leakage model for each subkey guess\n",
    "        leakage = leakage_model_hamming_weight(\n",
    "            num_traces=len(plaintexts),\n",
    "            plaintexts=plaintexts,\n",
    "            subkey_guess=k,\n",
    "            target_byte=target_byte\n",
    "        )\n",
    "        # Compute the Pearson correlation between the leakage and the traces\n",
    "        correlation = pearson_correlation(leakage, traces)\n",
    "        max_cpa[k] = np.nanmax(np.abs(correlation))  # Store the highest correlation for this guess\n",
    "\n",
    "    # Find the best subkey guess and highest correlation value\n",
    "    best_guess = np.argmax(max_cpa)\n",
    "    cpa_ref = np.nanmax(max_cpa)\n",
    "\n",
    "    return best_guess, cpa_ref\n",
    "\n",
    "def leakage_model_hamming_weight_snr(num_traces: int, plaintexts: list | np.ndarray, subkey_guess: any, target_byte: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates hypothetical leakage using the damming distance leakage model. In this implementation the reference state\n",
    "    is the output of the sbox at index 0.\n",
    "\n",
    "    :param num_traces: The number of traces collected when measuring the observed leakage\n",
    "    :type num_traces: int\n",
    "    :param plaintexts: The array of plaintexts used to collect the observed leakage\n",
    "    :type plaintexts: list | np.ndarray\n",
    "    :param subkey_guess: the subkey guess\n",
    "    :type subkey_guess: any\n",
    "    :param target_byte: the target byte of the key\n",
    "    :type target_byte: int\n",
    "    :return: numpy array of the hypothetical leakage\n",
    "    :rtype: np.ndarray\n",
    "    :Authors: Samuel Karkache (swkarkache@wpi.edu)\n",
    "    \"\"\"\n",
    "    leakage = np.empty(num_traces, dtype=object)\n",
    "\n",
    "    for i in range(num_traces):\n",
    "        leakage[i] = bin(Sbox[subkey_guess[i][target_byte] ^ plaintexts[i][target_byte]]).count('1')\n",
    "\n",
    "    return leakage\n",
    "\n",
    "def sbox_snr(num_traces: int, plaintexts: list | np.ndarray, subkey_guess: any, target_byte: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates hypothetical leakage using the damming distance leakage model. In this implementation the reference state\n",
    "    is the output of the sbox at index 0.\n",
    "\n",
    "    :param num_traces: The number of traces collected when measuring the observed leakage\n",
    "    :type num_traces: int\n",
    "    :param plaintexts: The array of plaintexts used to collect the observed leakage\n",
    "    :type plaintexts: list | np.ndarray\n",
    "    :param subkey_guess: the subkey guess\n",
    "    :type subkey_guess: any\n",
    "    :param target_byte: the target byte of the key\n",
    "    :type target_byte: int\n",
    "    :return: numpy array of the hypothetical leakage\n",
    "    :rtype: np.ndarray\n",
    "    :Authors: Samuel Karkache (swkarkache@wpi.edu)\n",
    "    \"\"\"\n",
    "    leakage = np.empty(num_traces, dtype=object)\n",
    "\n",
    "    for i in range(num_traces):\n",
    "        leakage[i] = Sbox[subkey_guess[i][target_byte] ^ plaintexts[i][target_byte]]\n",
    "\n",
    "    return leakage\n",
    "\n",
    "def plot_heatmap(heatmap_values, text, anno=False, cba=True, squar=True):\n",
    "    \"\"\"\n",
    "    Plots a heatmap using seaborn.\n",
    "\n",
    "    Parameters:\n",
    "    - heatmap_values: A 2D array or matrix of values to display in the heatmap.\n",
    "    - text: Title for the heatmap.\n",
    "    - anno: Boolean flag to display annotations on the heatmap (default is False).\n",
    "    - cba: Boolean flag to display the color bar (default is True).\n",
    "    - squar: Boolean flag to make the plot square-shaped (default is True).\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Create a heatmap using seaborn\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(heatmap_values, annot=anno, cbar=cba, square=squar)\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.title(text)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "    \n",
    "def save_mat(matr, file_name):\n",
    "    \"\"\"\n",
    "    Saves a given matrix to a .mat file.\n",
    "\n",
    "    Parameters:\n",
    "    - matr: The matrix to be saved (should be a numpy array).\n",
    "    - file_name: The name of the file to save the matrix to (including the .mat extension).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Save the matrix to a .mat file using scipy's savemat function\n",
    "    scipy.io.savemat(file_name, {'matrix': matr})    \n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def save_fig(filename):\n",
    "    \"\"\"\n",
    "    Saves the current figure to a specified file in SVG format.\n",
    "\n",
    "    Parameters:\n",
    "    - filename: The name of the file (should include the file extension, e.g., '.svg').\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Save the current figure to the specified file with SVG format\n",
    "    plt.gcf().savefig(filename, format=\"svg\", bbox_inches=\"tight\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def plot_SNR_heatmap_byte_bp(test, pt_exp, num, target_byte=0, grid_size=5, SNR_type=\"BYTE\"):\n",
    "    \"\"\"\n",
    "    Calculate the Signal-to-Noise Ratio (SNR) for a grid and plot the results.\n",
    "\n",
    "    Parameters:\n",
    "    - test: An object that handles data retrieval.\n",
    "    - pt_exp: Experiment data handler for the plaintexts and keys.\n",
    "    - num: The number of traces to consider.\n",
    "    - target_byte: The byte to target in the S-box (default is 0).\n",
    "    - grid_size: The size of the grid (default is 5x5).\n",
    "    - SNR_type: The type of SNR to compute (\"BYTE\", \"HW\", or \"FULL\").\n",
    "    \n",
    "    Returns:\n",
    "    - CEMA_values: The calculated SNR values for each grid location.\n",
    "    \"\"\"\n",
    "    keys = pt_exp.get_dataset(\"keys\").read_data(0, num)\n",
    "    plaintexts = pt_exp.get_dataset(\"plaintexts\").read_data(0, num)\n",
    "    \n",
    "    # Initialize the list to store SNR values\n",
    "    CEMA_values = []\n",
    "\n",
    "    # Select the appropriate SNR calculation based on the SNR_type\n",
    "    if SNR_type == \"BYTE\":\n",
    "        labels = sbox_snr(num_traces=num, plaintexts=plaintexts, subkey_guess=keys, target_byte=target_byte)\n",
    "    elif SNR_type == \"HW\":\n",
    "        labels = leakage_model_hamming_weight_snr(num_traces=num, plaintexts=plaintexts, subkey_guess=keys, target_byte=target_byte)\n",
    "    elif SNR_type == \"FULL\":\n",
    "        labels = Sbox[plaintexts ^ keys]\n",
    "        labels = labels[:, 1]\n",
    "    else:\n",
    "        print(\"Incorrect SNR type\")\n",
    "        return -1\n",
    "\n",
    "    # Get unique labels for the SNR calculation\n",
    "    labelsUnique = np.unique(labels)\n",
    "\n",
    "    # Calculate SNR for each grid location\n",
    "    for i in range(grid_size):\n",
    "        for j in trange(grid_size):\n",
    "            sorted_labels = {k: [] for k in labelsUnique}  # Initialize sorted labels dictionary\n",
    "            print(f\"loop_{i}_{j}\")\n",
    "\n",
    "            traces = test.get_dataset(f\"random_{i}_{j}\").read_data(0, num)\n",
    "            \n",
    "            # Organize the traces based on the labels\n",
    "            for index, label in enumerate(labels):\n",
    "                sorted_labels[label].append(np.array(traces[index]))\n",
    "\n",
    "            # Calculate the SNR for the current grid point\n",
    "            snr_value = signal_to_noise_ratio(sorted_labels)\n",
    "\n",
    "            # Store the SNR value\n",
    "            CEMA_values.append(snr_value)\n",
    "\n",
    "    return CEMA_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349fcbb",
   "metadata": {},
   "source": [
    "# deprecated functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc6117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "        \n",
    "# def Grid_Tracing(X_range,Y_range,X_number_of_step,Y_number_of_step,X,Y,Z,interface,sco,number_of_traces):\n",
    "#     cordinate_traces={}\n",
    "#     X_moment=0\n",
    "#     Y_moment=0\n",
    "#     traces = [capture_nopt(sco,num_of_samples=500) for i in trange(number_of_traces)]\n",
    "#     cordinate = (X_moment, Y_moment)\n",
    "#     cordinate_traces[cordinate] = traces\n",
    "#     X_start_position=X.get_actual_position()\n",
    "#     Y_start_position=Y.get_actual_position()\n",
    "#     print(f\"Starting Postion - ({X_start_position},{Y_start_position})\")\n",
    "#     while Y_moment<=Y_number_of_step:\n",
    "#         X_initial_position=X.get_actual_position()\n",
    "#         Y_initial_position=Y.get_actual_position()\n",
    "\n",
    "#         for _ in range(X_number_of_step):\n",
    "#             X.move_by(X_range)\n",
    "#             print(f'Moving X to {X_initial_position + X_range}')\n",
    "\n",
    "#             while X.get_actual_position() != X_initial_position + X_range:\n",
    "\n",
    "\n",
    "#                 time.sleep(0.1)\n",
    "#             traces = [capture_nopt(sco,num_of_samples=500) for i in trange(number_of_traces)]\n",
    "#             X_moment += 1\n",
    "#             cordinate = (X_moment, Y_moment)\n",
    "#             cordinate_traces[cordinate] = traces\n",
    "#             X_initial_position=X.get_actual_position()\n",
    "\n",
    "#         if Y_moment==Y_number_of_step:\n",
    "#             break\n",
    "#         Y.move_by(Y_range)\n",
    "#         Y_moment+=1\n",
    "#         print(f'Moving Y to {Y_initial_position + Y_range}')\n",
    "#         while Y.get_actual_position() != Y_initial_position + Y_range:\n",
    "\n",
    "#             time.sleep(0.1)\n",
    "#         cordinate = (X_moment, Y_moment)\n",
    "#         cordinate_traces[cordinate] = [capture_nopt(sco,num_of_samples=500) for i in trange(number_of_traces)]\n",
    "#         Y_initial_position=Y.get_actual_position()\n",
    "\n",
    "#         for _ in range(X_number_of_step):\n",
    "#             X.move_by(-X_range)\n",
    "#             print(f'Moving X to {X_initial_position - X_range}')\n",
    "#             while X.get_actual_position() != X_initial_position - X_range:\n",
    "\n",
    "#                 time.sleep(0.1)\n",
    "#             traces = [capture_nopt(sco,num_of_samples=500) for i in trange(number_of_traces)]\n",
    "#             X_moment -= 1\n",
    "#             cordinate = (X_moment, Y_moment)\n",
    "#             cordinate_traces[cordinate] = traces\n",
    "#             X_initial_position=X.get_actual_position()\n",
    "#         if Y_moment==Y_number_of_step:\n",
    "#             break\n",
    "#         # Move down 1 step\n",
    "#         Y.move_by(Y_range)\n",
    "#         Y_moment += 1\n",
    "#         print(f'Moving Y to {Y_initial_position + Y_range}')\n",
    "#         while Y.get_actual_position() != Y_initial_position + Y_range:\n",
    "#             time.sleep(0.1)\n",
    "\n",
    "#         cordinate = (X_moment, Y_moment)\n",
    "#         cordinate_traces[cordinate] = [capture_nopt(sco,num_of_samples=500) for i in trange(number_of_traces)]\n",
    "#         Y_initial_position=Y.get_actual_position()\n",
    "    \n",
    "#     X.move_to(X_start_position)\n",
    "#     while X.get_actual_position() != X_start_position:\n",
    "#         time.sleep(0.1)\n",
    "#     Y.move_to(Y_start_position)\n",
    "#     while Y.get_actual_position() != Y_start_position:\n",
    "#         time.sleep(0.1)\n",
    "#     print(f\"Final Postion - ({X.get_actual_position()},{Y.get_actual_position()})\")\n",
    "\n",
    "#     return cordinate_traces\n",
    "\n",
    "\n",
    "def plot_SNR_heatmap_byte(test, pt_exp, num, target_byte=0, grid_size=5):\n",
    "    \"\"\"\n",
    "    Calculate Signal-to-Noise Ratio (SNR) for a grid and plot the results as a heatmap.\n",
    "\n",
    "    Parameters:\n",
    "    - test: An object that provides methods to retrieve traces.\n",
    "    - pt_exp: Experiment data handler containing keys and plaintexts.\n",
    "    - num: Number of traces to consider.\n",
    "    - target_byte: The target byte of the AES S-box (default is 0).\n",
    "    - grid_size: The size of the grid (default is 5x5).\n",
    "    \n",
    "    Returns:\n",
    "    - CEMA_values_rotated: The rotated matrix of SNR values.\n",
    "    - Log-transformed CEMA values: Log-transformed SNR values for better visualization.\n",
    "    \"\"\"\n",
    "    # Load keys and plaintexts from the experiment data\n",
    "    keys = pt_exp.get_dataset(\"keys\").read_data(0, num)\n",
    "    plaintexts = pt_exp.get_dataset(\"plaintexts\").read_data(0, num)\n",
    "\n",
    "    # Initialize the CEMA values matrix\n",
    "    CEMA_values = np.zeros((grid_size, grid_size))\n",
    "\n",
    "    # Calculate the SNR labels based on the chosen target byte\n",
    "    labels = sbox_snr(num_traces=num, plaintexts=plaintexts, subkey_guess=keys, target_byte=target_byte)\n",
    "    labelsUnique = np.unique(labels)\n",
    "\n",
    "    # Iterate over the grid and compute the SNR for each point\n",
    "    for i in range(grid_size):\n",
    "        for j in trange(grid_size):\n",
    "            sorted_labels = {k: [] for k in labelsUnique}  # Initialize dictionary to hold sorted labels\n",
    "            print(f\"Processing grid point ({i}, {j})\")\n",
    "\n",
    "            traces = test.get_dataset(f\"random_{i}_{j}\").read_data(0, num)\n",
    "\n",
    "            # Organize the traces based on the labels\n",
    "            for index, label in enumerate(labels):\n",
    "                sorted_labels[label].append(np.array(traces[index]))\n",
    "\n",
    "            # Calculate the SNR for the current grid point\n",
    "            snr_value = signal_to_noise_ratio(sorted_labels)\n",
    "\n",
    "            # Store the maximum SNR value for this grid point\n",
    "            CEMA_values[i, j] = np.nanmax(np.abs(snr_value))\n",
    "\n",
    "    # Rotate the CEMA values matrix\n",
    "    CEMA_values_rotated = np.rot90(CEMA_values, k=3)  # Rotate by 90 degrees clockwise (k=3)\n",
    "\n",
    "    # Plot the heatmap using seaborn\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(CEMA_values_rotated, annot=True, cbar=True, square=True)\n",
    "\n",
    "    # Adding labels and title to the plot\n",
    "    plt.title(\"Heatmap of SNR Byte\")\n",
    "    plt.xlabel(\"j\")\n",
    "    plt.ylabel(\"i\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Return the rotated CEMA values and their log-transformed values\n",
    "    return CEMA_values_rotated, 10 * np.log10(CEMA_values_rotated)\n",
    "\n",
    "def plot_SNR_heatmap_hw_byte(test, pt_exp, num, target_byte=0, grid_size=5):\n",
    "    \"\"\"\n",
    "    Calculate the Hamming Weight-based SNR for a grid and plot the results as a heatmap.\n",
    "\n",
    "    Parameters:\n",
    "    - test: An object that has a `calculate_t_test` method to compute the t-statistics.\n",
    "    - pt_exp: Experiment data handler containing keys and plaintexts.\n",
    "    - num: Number of traces to consider.\n",
    "    - target_byte: The target byte of the AES S-box (default is 0).\n",
    "    - grid_size: The size of the grid (default is 5x5).\n",
    "    \n",
    "    Returns:\n",
    "    - CEMA_values_rotated: The rotated matrix of SNR values.\n",
    "    - Log-transformed CEMA values: Log-transformed SNR values for better visualization.\n",
    "    \"\"\"\n",
    "    # Load keys and plaintexts from the experiment data\n",
    "    keys = pt_exp.get_dataset(\"keys\").read_data(0, num)\n",
    "    plaintexts = pt_exp.get_dataset(\"plaintexts\").read_data(0, num)\n",
    "\n",
    "    # Initialize matrices for storing SNR values and guesses\n",
    "    CEMA_values = np.zeros((grid_size, grid_size))\n",
    "    CEMA_guesses = np.zeros((grid_size, grid_size))  # Not used currently, can be useful for future extensions\n",
    "\n",
    "    # Calculate SNR labels using the Hamming Weight leakage model\n",
    "    labels = leakage_model_hamming_weight_snr(num_traces=num, plaintexts=plaintexts, subkey_guess=keys, target_byte=target_byte)\n",
    "    labelsUnique = np.unique(labels)\n",
    "\n",
    "    # Iterate over the grid to compute SNR for each point\n",
    "    for i in range(grid_size):\n",
    "        for j in trange(grid_size):\n",
    "            sorted_labels = {k: [] for k in labelsUnique}  # Initialize the dictionary for sorted labels\n",
    "            print(f\"Processing grid point ({i}, {j})\")\n",
    "\n",
    "            traces = test.get_dataset(f\"random_{i}_{j}\").read_data(0, num)\n",
    "\n",
    "            # Organize traces based on the labels\n",
    "            for index, label in enumerate(labels):\n",
    "                sorted_labels[label].append(np.array(traces[index]))\n",
    "\n",
    "            # Calculate the SNR for the current grid point\n",
    "            snr_value = signal_to_noise_ratio(sorted_labels)\n",
    "\n",
    "            # Store the maximum SNR value for this grid point\n",
    "            CEMA_values[i, j] = np.nanmax(np.abs(snr_value))\n",
    "\n",
    "    # Rotate the CEMA values matrix\n",
    "    CEMA_values_rotated = np.rot90(CEMA_values, k=3)  # Rotate by 90 degrees clockwise (k=3)\n",
    "\n",
    "    # Create a heatmap using seaborn\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(CEMA_values_rotated, annot=True, cbar=True, square=True)\n",
    "\n",
    "    # Adding labels and title to the plot\n",
    "    plt.title(\"Heatmap of SNR Byte HW\")\n",
    "    plt.xlabel(\"j\")\n",
    "    plt.ylabel(\"i\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Return the rotated CEMA values and their log-transformed values\n",
    "    return CEMA_values_rotated, 10 * np.log10(CEMA_values_rotated)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
